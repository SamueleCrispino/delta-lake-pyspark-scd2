# sul tuo laptop
ssh-keygen -t ed25519 -C "samuele@unimib"    # premi invio e non mettere passphrase se non vuoi
# copia chiave pubblica
cat ~/.ssh/id_ed25519.pub


# Master Node:
ssh Crispinoadmin@13.93.5.130


ssh Crispinoadmin@13.93.4.92        ok
ssh Crispinoadmin@13.93.4.169       ok       
ssh Crispinoadmin@52.174.63.82      ok
ssh Crispinoadmin@13.93.4.139       ok
ssh Crispinoadmin@13.93.4.240       ok
ssh Crispinoadmin@13.93.5.195       ok

chmod +x ~/install_prereqs_worker.sh && sudo ~/install_prereqs_worker.sh


# dal manager
source /etc/profile.d/spark.sh
# avvia master (default su spark://<manager_ip>:7077)
sudo $SPARK_HOME/sbin/start-master.sh

# prendi master url dalla web UI o log
# es: spark://13.93.5.130:7077

# avvia worker locali (o su workers esegui start-slave.sh target master)
# su ogni worker:
$SPARK_HOME/sbin/start-slave.sh spark://13.93.5.130:7077
# oppure su manager avvia anche workers remoti via ssh se vuoi



# info su master:
cat /opt/spark/logs/spark-root-org.apache.spark.deploy.worker.Worker-1-VM-2.out

--> aprire tunnel:
ssh -L 8080:10.0.1.7:8080 Crispinoadmin@13.93.5.130


cd /opt/spark/conf
sudo cp /opt/spark/conf/workers.template /opt/spark/conf/workers

Genera la chiave SSH sul master: Se non hai già una coppia di chiavi, creane una.
ssh-keygen -t rsa -P "" -f ~/.ssh/id_rsa

# Copy to all workers
ssh-copy-id -i ~/.ssh/id_rsa.pub Crispinoadmin@13.93.4.92
ssh-copy-id -i ~/.ssh/id_rsa.pub Crispinoadmin@13.93.4.169
ssh-copy-id -i ~/.ssh/id_rsa.pub Crispinoadmin@52.174.63.82
ssh-copy-id -i ~/.ssh/id_rsa.pub Crispinoadmin@13.93.4.139
ssh-copy-id -i ~/.ssh/id_rsa.pub Crispinoadmin@13.93.4.240
ssh-copy-id -i ~/.ssh/id_rsa.pub Crispinoadmin@13.93.5.195

cd /opt/spark/sbin
# cambiare proprietario cartella logs
sudo chown -R Crispinoadmin:Crispinoadmin /opt/spark/logs

for worker in $(cat workers.txt); do ssh Crispinoadmin@$worker "sudo mkdir -p /opt/spark/logs && sudo chown -R Crispinoadmin:Crispinoadmin /opt/spark/logs"; done

cd /opt/spark/conf
sudo cp spark-env.sh.template spark-env.sh
nano spark-env.sh  # <--- export SPARK_WORKER_DIR=/data/spark_work
sudo mkdir -p /data/spark_work
sudo chown -R Crispinoadmin:Crispinoadmin /data/spark_work


for worker in $(cat workers.txt); do scp /opt/spark/conf/spark-env.sh Crispinoadmin@$worker:/tmp/; done
for worker in $(cat workers.txt); do ssh Crispinoadmin@$worker "sudo mv /tmp/spark-env.sh /opt/spark/conf/"; done


ssh Crispinoadmin@13.93.4.92   "cat /opt/spark/conf/spark-env.sh" | grep -i /data/spark_work
ssh Crispinoadmin@13.93.4.169  "cat /opt/spark/conf/spark-env.sh" | grep -i /data/spark_work
ssh Crispinoadmin@52.174.63.82 "cat /opt/spark/conf/spark-env.sh" | grep -i /data/spark_work
ssh Crispinoadmin@13.93.4.139  "cat /opt/spark/conf/spark-env.sh" | grep -i /data/spark_work
ssh Crispinoadmin@13.93.4.240  "cat /opt/spark/conf/spark-env.sh" | grep -i /data/spark_work
ssh Crispinoadmin@13.93.5.195  "cat /opt/spark/conf/spark-env.sh" | grep -i /data/spark_work


ssh Crispinoadmin@13.93.4.92   "ls -ld /data/spark_work"
ssh Crispinoadmin@13.93.4.169  "ls -ld /data/spark_work"
ssh Crispinoadmin@52.174.63.82 "ls -ld /data/spark_work"
ssh Crispinoadmin@13.93.4.139  "ls -ld /data/spark_work"
ssh Crispinoadmin@13.93.4.240  "ls -ld /data/spark_work"
ssh Crispinoadmin@13.93.5.195  "ls -ld /data/spark_work"


#### Soluzione: Impostare la directory di lavoro in locale

sudo nano /opt/spark/conf/spark-env.sh # <-- export SPARK_WORKER_DIR=/tmp/spark_work

ssh Crispinoadmin@13.93.4.92   "cat /opt/spark/conf/spark-env.sh" | grep -i /tmp/spark_work
ssh Crispinoadmin@13.93.4.169  "cat /opt/spark/conf/spark-env.sh" | grep -i /tmp/spark_work
ssh Crispinoadmin@52.174.63.82 "cat /opt/spark/conf/spark-env.sh" | grep -i /tmp/spark_work
ssh Crispinoadmin@13.93.4.139  "cat /opt/spark/conf/spark-env.sh" | grep -i /tmp/spark_work
ssh Crispinoadmin@13.93.4.240  "cat /opt/spark/conf/spark-env.sh" | grep -i /tmp/spark_work
ssh Crispinoadmin@13.93.5.195  "cat /opt/spark/conf/spark-env.sh" | grep -i /tmp/spark_work

/opt/spark/sbin/start-all.sh


####################### CREATING MANAGER ENV ##############################
python3 -m venv /data/project/venv
source /data/project/venv/bin/activate
pip install --upgrade pip
pip install pyspark==3.3.2
pip install delta-spark==3.1.0
pip install pandas pyarrow



#######################  CLONING SPARK REPO
cd /data  --> you don't have to create it, it must already exist!! "/data" != "data"
git clone <il_tuo_repo_git> project || true
# o copia via scp
# assicurati che i permessi siano giusti
sudo chown -R Crispinoadmin:Crispinoadmin /data/project


##### RUNNING
# dal manager
source /data/delta-lake-pyspark-scd2/venv/bin/activate  # opzionale, ma utile per aver python nel PATH

export PYSPARK_PYTHON=/data/delta-lake-pyspark-scd2/venv/bin/python
export PYSPARK_DRIVER_PYTHON=/data/delta-lake-pyspark-scd2/venv/bin/python

# spark-submit con jar delta (usiamo --packages per scaricarlo)
$SPARK_HOME/bin/spark-submit \
  --master spark://13.93.5.130:7077 \
  --deploy-mode client \
  --conf "spark.sql.legacy.timeParserPolicy=CORRECTED" \
  --conf "spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog" \
  --packages io.delta:delta-spark_2.12:3.1.0 \
  /data/delta-lake-pyspark-scd2/header_etl.py \
  --read-path /data/delta-lake-pyspark-scd2/crm_with_event_time/header/header_20230122.csv \
  --write-path /data/delta/landing/header

# NOTA: Importante: usa PYSPARK_PYTHON per far usare il python del virtualenv. --packages farà scaricare il JAR delta-spark al primo run.


####### Logging, audit e raccolta metriche (semplice ma efficace)
Implementa questo pattern in uno script wrapper run_job_wrapper.sh che:

registra start_ts e end_ts

lancia spark-submit redirectando stdout/stderr a un log file

legge counts da Delta (inserted_count, closed_count) come fai già

scrive una riga CSV/JSON in /data/project/runs/runs_metrics.csv con: run_id, start_ts, end_ts, duration_s, nodes_used, staged_count, inserted_count, closed_count, staged_count, error_flag, spark_ui_url

Esempio minimale:

#!/usr/bin/env bash
START=$(date -u +"%Y-%m-%dT%H:%M:%SZ")
RUN_ID=$(date +"%Y%m%d%H%M%S")
LOG="/data/project/runs/log_${RUN_ID}.log"

# lancia job
spark-submit ... > ${LOG} 2>&1
RC=$?

END=$(date -u +"%Y-%m-%dT%H:%M:%SZ")

# estrai metriche via pyspark oppure via spark-sql
python3 <<PY
from pyspark.sql import SparkSession
s = SparkSession.builder.appName('metrics_reader').getOrCreate()
df = s.read.format('delta').load('/data/delta/landing/header')
inserted = df.filter(df.batch_id == '${RUN_ID}').count()
print("${RUN_ID},${START},${END},${inserted},${RC}")
s.stop()
PY



####### 10) Piano di test (cosa misuriamo e come)

Base: tieni un README che descrive i test. Ecco come eseguire i tre esperimenti del prof:

Test A — scala numero nodi (stessa volume)

Prepara un dataset con volume V (es. 10M record — puoi generarlo duplicando file o con script).

Configurazioni:

Config 1: manager + 2 workers (usa subset di worker)

Config 2: manager + 4 workers

Config 3: manager + 8 workers (se disponibili)

Per ogni config:

riavvia workers necessari (start-slave.sh sui worker)

lancia lo stesso job (spark-submit) e misura: tempo ETL (start/end), peak CPU/mem per nodo (puoi usare dstat o vmstat), inserted_count.

registra metriche.

Test B — scala volume (stessi nodi)

Fissa la config (es. manager + 4 workers).

Esegui job con volumi V1, V2, V3 (es. 1M, 5M, 20M).

Misura tempo ingest e query su Delta (es. SELECT count(*) WHERE valid_from = X).

Test C — change point / time travel

Carica dati secondo schema A (prima versione).

Esegui query SELECT ... e registra risultati / latenza.

Cambia schema (aggiungi colonna, oppure rimuovi/rename) e scrivi nuovo batch con nuovo schema.

Esegui le stesse query su schema vecchio (time travel su Delta: versionAsOf o timestampAsOf) e su schema nuovo.

Misura tempi e verifica coerenza*.

* Nota: per time travel devi usare Delta table (che state usando) e usare il protocollo: spark.read.format("delta").option("versionAsOf", N).load(path) oppure option("timestampAsOf", "2023-...").


